{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Transition Dependency Parser in PyTorch\n",
    "\n",
    "In this problem set, you will implement a deep transition dependency parser in [PyTorch](https://pytorch.org).\n",
    "\n",
    "You will:\n",
    "\n",
    "- Implement an arc-standard transition-based dependency parser in PyTorch\n",
    "- Implement neural network components for choosing actions and combining stack elements\n",
    "- Train your network to parse English and Norwegian sentences\n",
    "- Implement techniques to improve your parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [conda](https://docs.conda.io/en/latest/miniconda.html), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy/user/install.html)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this assignment\n",
    "\n",
    "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
    "- Most of your coding will be in the python source files in the directory ```mynlplib```.\n",
    "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at https://nose.readthedocs.io/en/latest/usage.html.\n",
    "- You may want to add more tests, but that is completely optional.\n",
    "- **To submit this assignment, run the script ```make-submission.sh```, and submit the tarball ```pset3-submission.tgz``` on Canvas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as ag\n",
    "\n",
    "import nose\n",
    "import numpy as np\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "numpy: 1.19.1\n",
      "nose: 1.3.7\n",
      "torch: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('torch: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mynlplib.parsing as parsing\n",
    "import mynlplib.data_tools as data_tools\n",
    "import mynlplib.constants as consts\n",
    "import mynlplib.evaluation as evaluation\n",
    "import mynlplib.utils as utils\n",
    "import mynlplib.feat_extractors as feat_extractors\n",
    "import mynlplib.neural_net as neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "reload(data_tools)\n",
    "en_dataset = data_tools.Dataset(consts.EN_TRAIN_FILE, consts.EN_DEV_FILE, consts.EN_TEST_FILE)\n",
    "nr_dataset = data_tools.Dataset(consts.NR_TRAIN_FILE, consts.NR_DEV_FILE, consts.NR_TEST_FILE)\n",
    "\n",
    "# Assign each word a unique index, including two special tokens needed for parsing logic\n",
    "word_to_ix_en = { word: i for i, word in enumerate(en_dataset.vocab) }\n",
    "word_to_ix_nr = { word: i for i, word in enumerate(nr_dataset.vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants to keep around\n",
    "LSTM_NUM_LAYERS = 1\n",
    "TEST_EMBEDDING_DIM = 4\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "STACK_EMBEDDING_DIM = 100\n",
    "NUM_FEATURES = 3\n",
    "\n",
    "# Hyperparameters\n",
    "ETA_0 = 0.01\n",
    "DROPOUT = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Overview of the Parser\n",
    "Be sure that you have reviewed Eisenstein Ch. 11.3 on transition-based dependency parsing, and are familiar with the relevant terminology.\n",
    "Parsing will proceed as follows:\n",
    "* Initialize your parsing stack and input buffer.\n",
    "* At each step, until the parse is done:\n",
    "  * Extract some features.  We will start with simple features, but these can be anything: words in the sentence, the configuration of the stack, the configuration of the input buffer, the previous action, etc.\n",
    "  * Send these features through a feed-forward (FF) network to get a probability distribution over actions (`SHIFT`, `ARC_L`, `ARC_R`).  The next action you choose is the one with the highest probability.\n",
    "  * If the action is an arc- operation, you use a neural network to combine the two items in the operation and get a dense output to place back on the input buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key classes you will fill in code for are:\n",
    "* Feature extraction in `feat_extractors.py`\n",
    "* The `ParserState` class, which keeps track of the input buffer and parse stack, and offers a public interface for doing the parsing actions to update the state\n",
    "* The `TransitionParser` class, which is a PyTorch module where the core parsing logic resides, in `parsing.py`.\n",
    "* The neural network components in `neural_net.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network components are compartmentalized as follows:\n",
    "* `TransitionParser`, the base component that contains and coordinates the other substitutable components\n",
    "\n",
    "* Embedding Lookup: You will implement three flavors of embeddings. These embeddings are used to initialize the input buffer, and will be shifted on the stack / serve as inputs to the combiner networks.\n",
    "  - `VanillaWordEmbedding` just gets embeddings from a lookup table.\n",
    "  - `BiLSTMWordEmbedding` will run a sequence model in both directions over the sentence. The hidden state at step t is the embedding for the `t`-th word of the sentence.\n",
    "  - `SuffixAndWordEmbedding` gets embeddings for words as in the vanilla embeddings, and also gets embeddings for word suffixes, and concatenates them together.\n",
    "* Action Choosing: You will implement two action choosing components:\n",
    "  - `FFActionChooser` is a simple feed-forward neural network that outputs log probabilities over the three actions given the extracted features as input.\n",
    "  - `LSTMActionChooser` applies a sequence model that takes the hidden state of the previous action decision as input.\n",
    "\n",
    "* Combiners: You will implement two combiners, which are the network components that take the two embeddings of the items in an arc- operation and creates a single vector.\n",
    "  - `FFCombiner` takes the two input embeddings and gives a dense output.\n",
    "  - `LSTMCombiner` applies a sequence model, where the output embedding is the hidden state of the next timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how the input buffer and stack look at each step of a parse, up to the first arc.  The input sentence is \"the dog ran away\".  Our action chooser network takes the top element of the stack, the top element of the input buffer, plus a one-token \"lookahead\" in the input buffer.  $C(x,y)$ refers to calling our combiner network on arguments $x, y$.  Also let $A$ be the set of actions: $\\{ \\text{SHIFT}, \\text{ARC-L}, \\text{ARC-R} \\}$, and let $q_w$ be the embedding for word $w$.\n",
    "\n",
    "1. \n",
    "  * Input Buffer: $\\left[ q_\\text{the}, q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
    "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{ROOT}, q_\\text{the}, \\overbrace{q_\\text{dog}}^\\text{lookahead}) \\Rightarrow \\text{SHIFT}$\n",
    "  \n",
    "2.\n",
    "  * Input Buffer: $\\left[ q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT}, q_\\text{the} \\right]$\n",
    "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{the}, q_\\text{dog}, q_\\text{ran}) \\Rightarrow \\text{ARC-L}$\n",
    "  \n",
    "3.\n",
    "  * Input Buffer: $\\left[C(q_\\text{dog}, q_\\text{the}), q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
    "  \n",
    "This is a partial picture of parsing - we keep more than just the embedding on the stack and input buffer.  We also keep the word and its position in the sentence so that when we create an arc, we know what edge was just created.\n",
    "So, for example, the initial input buffer really looks like\n",
    "\n",
    "$$ \\left[ (\\text{the}, 0, q_\\text{the}), (\\text{dog}, 1, q_\\text{dog}), (\\text{ran}, 2, q_\\text{ran}), (\\text{away}, 3, q_\\text{away}), (\\text{END-INPUT}, 4, q_\\text{END-INPUT}) \\right] $$\n",
    "\n",
    "Before beginning, I recommend completing the parse by hand, drawing the input buffer and stack at each step, and explicity listing the arguments to the action chooser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Managing and Updating the Parser State (12 points)\n",
    "\n",
    "In this part of the assignment, you will work with the ParserState class, that keeps track of the parser's input buffer and stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.1: Implementing Arc\n",
    "\n",
    "#### 1.1a: Get arc components (2 points)\n",
    "You will implement the generalized arc- operation of the `ParserState` in `parsing.py`, in the function `_arc`, in two parts.\n",
    "\n",
    "First, fill in `_get_arc_components` in `parsing.py` to select the head and modifier according to the action passed in. This method should also remove the items from the stack and input buffer.\n",
    "Arc actions follow the arc-standard procedure, from Eisenstein Ch. 11.3.1.\n",
    "\n",
    "- **Test**: ` test_parser.py:test_get_arc_components_d1_1a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "test_sentence = \"The man ran away\".split()\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>', 'The', 'man']\n",
      "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "StackEntry(headword='ran', headword_pos=2, embedding=None) StackEntry(headword='man', headword_pos=1, embedding=None)\n",
      "StackEntry(headword='The', headword_pos=0, embedding=None) StackEntry(headword='away', headword_pos=3, embedding=None)\n"
     ]
    }
   ],
   "source": [
    "parser_state.shift()\n",
    "parser_state.shift()\n",
    "print(parser_state)\n",
    "\n",
    "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_L)\n",
    "print(head, modifier)\n",
    "\n",
    "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_R)\n",
    "print(head, modifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1b: Create the arc (2 points)\n",
    "Now, fill in `_create_arc` in `parsing.py` to use the `ParserState`'s `combiner` component to **combine** the passed in head and modifier, put the combination on the input buffer, and create a new dependency graph edge. At this point, we are just using a dummy combiner so we can test the logic.\n",
    "\n",
    "You will want to familiarize yourself with the `StackEntry` and `DepGraphEdge` objects used by the `ParserState` object for this one.\n",
    "\n",
    "- **Test**: ` test_parser.py:test_create_arc_d1_1b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Stack: ['<ROOT>', 'The']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "First arc: Head: ('man', 1), Modifier: ('The', 0) \n",
      "\n",
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Second arc: Head: ('ran', 2), Modifier: ('man', 1) \n",
      "\n",
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "\n",
    "print(parser_state)\n",
    "\n",
    "parser_state.shift()\n",
    "print(parser_state)\n",
    "\n",
    "arc = parser_state.arc_left()\n",
    "print(\"First arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
    "print(parser_state)\n",
    "\n",
    "parser_state.shift()\n",
    "arc = parser_state.arc_left()\n",
    "print(\"Second arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
    "print(parser_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.2: Parser Terminating Condition (4 points)\n",
    "In this short (one line) deliverable, implement `done_parsing()` in `ParserState`.  Think about what the input buffer and stack look like at the end of a parse.\n",
    "\n",
    "- **Test**: `test_parsing.py:test_stack_terminating_cond_d1_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.arc_left()\n",
    "parser_state.shift()\n",
    "parser_state.arc_left()\n",
    "\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.arc_right()\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.arc_right()\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.shift()\n",
    "print(parser_state.done_parsing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.3: Validating parser actions (4 points)\n",
    "Implement the `_validate_action` method in `parsing.TransitionParser`. This will be used in the prediction setting, when the gold standard is not available. We need to ensure that any action we take is legal. Here are the rules:\n",
    "\n",
    "- You cannot shift when the input buffer has <= 2 items on it (including the end of input token), UNLESS the stack is empty.\n",
    "  - **In this case, do `ARC_R` by default.**\n",
    "- You cannot do an arc- operation when the stack is empty (this will happen after creating an arc with ROOT).\n",
    "  - **In this case, do `SHIFT` by default.**\n",
    "- You cannot do an arc-left operation when the root token is on top of the stack.\n",
    "  - **In this case, do `SHIFT` or `ARC-R` depending on the state of the input buffer.**\n",
    "  \n",
    "**Test:**\n",
    "- `test_parser.py:test_validate_action_d1_3`\n",
    "\n",
    "**Make sure you pass the test before you move on. The code blocks below are not meant to be comprehensive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "ix_to_action = consts.Actions.ix_to_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: ARC_L, Valid action: SHIFT\n",
      "\n",
      "Stack: ['<ROOT>', 'The']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: ARC_L, Valid action: ARC_L\n",
      "\n",
      "Stack: ['<ROOT>', 'The', 'man', 'ran']\n",
      "Input Buffer: ['away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: SHIFT, Valid action: ARC_R\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser_state)\n",
    "act_to_do = consts.Actions.ARC_L\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
    "\n",
    "parser_state.shift()\n",
    "\n",
    "print(parser_state)\n",
    "act_to_do = consts.Actions.ARC_L\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.shift()\n",
    "\n",
    "print(parser_state)\n",
    "act_to_do = consts.Actions.SHIFT\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network for Action Decisions (12 points)\n",
    "In this part of the assignment, you will use PyTorch to create a neural network which examines the current state of the parse and makes the decision to either shift, arc left, or arc right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.1: Word Embedding Lookup (3 points)\n",
    "Implement the class `VanillaWordEmbedding` in `neural_net.py`\n",
    "([Docs for Pytorch embeddings](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))\n",
    "\n",
    "Hint: You will have to turn the input, which is a list of strings (the words in the sentence), into a format that your embedding lookup table can take. \n",
    "\n",
    "**Test:** `test_parser.py:test_word_embed_lookup_d2_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3 \n",
      "\n",
      "Embedding for 'natural':\n",
      " tensor([[0.6614, 0.2669, 0.0617, 0.6213]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_sentence = \"natural language processing\".split()\n",
    "test_word_to_ix = { \"natural\": 0, \"language\": 1, \"processing\": 2 }\n",
    "\n",
    "word_embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
    "embeds = word_embedder(test_sentence)\n",
    "print(type(embeds))\n",
    "print(len(embeds), \"\\n\")\n",
    "print(\"Embedding for 'natural':\\n {}\".format(embeds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.2: Feature Extraction (3 points)\n",
    "Fill in the `SimpleFeatureExtractor` class in `feat_extractors.py` to give the following 3 features as a list **in this order**:\n",
    "* The embedding of the top of the stack\n",
    "* The embedding of the first token in the input buffer\n",
    "* The embedding of the next token in the input buffer (one-token lookahead)\n",
    "\n",
    "If at this point you have not poked around `ParserState` to see how it stores the state, now would be a good time.\n",
    "\n",
    "**Test:** `test_parser.py:test_feature_extraction_d2_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'The':\n",
      " tensor([[ 0.4391,  1.1712,  1.7674, -0.0954]], grad_fn=<ViewBackward>)\n",
      "Embedding for 'Sound':\n",
      " tensor([[ 0.8657,  0.2444, -0.6629,  0.8073]], grad_fn=<ViewBackward>)\n",
      "Embedding for 'and' (from buffer lookahead):\n",
      " tensor([[ 0.0612, -0.6177, -0.7981, -0.1316]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(feat_extractors)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_sentence = \"The Sound and the Fury\".split()\n",
    "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
    "\n",
    "embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
    "embeds = embedder(test_sentence)\n",
    "\n",
    "state = parsing.ParserState(test_sentence, embeds, utils.DummyCombiner())\n",
    "\n",
    "state.shift()\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "feats = feat_extractor.get_features(state)\n",
    "\n",
    "print(\"Embedding for 'The':\\n {}\".format(feats[0]))\n",
    "print(\"Embedding for 'Sound':\\n {}\".format(feats[1]))\n",
    "print(\"Embedding for 'and' (from buffer lookahead):\\n {}\".format(feats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.3: Feedforward Network for Choosing Actions (3 points)\n",
    "Implement the class `neural_net.FFActionChooser` according to the specification.\n",
    "You will need to take the list of embeddings passed in (that come from your feature extractor) and concatenate them to one long row vector (size [1 x num actions])\n",
    "\n",
    "This network takes as input the features from your feature extractor, concatenates them, runs them through a feedforward network, and outputs log probabilities over actions.\n",
    "\n",
    "**Test:** `test_parser.py:test_action_chooser_d2_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2443, -0.8323, -1.2844]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "act_chooser = neural_net.FFActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES)\n",
    "feats = [ ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES) ] # make some dummy feature embeddings\n",
    "log_probs = act_chooser(feats)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.4: Network for Combining Stack Items (3 points)\n",
    "Implement the class `neural_net.FFCombiner` according to the specification. \n",
    "Recall that what this component does is take two embeddings, the head and modifier, during an arc- operation and output a combined embedding (of size [1 x embedding_dim]), which is then pushed back onto the input buffer during parsing.\n",
    "\n",
    "**Test:** `test_parser.py:test_combiner_d2_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4285, -0.1363,  0.4046,  0.6006]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "combiner = neural_net.FFCombiner(TEST_EMBEDDING_DIM)\n",
    "\n",
    "# Again, make dummy inputs\n",
    "head_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
    "modifier_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
    "combined = combiner(head_feat, modifier_feat)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Return of the Parser (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.1: Parser Training Code (8 points)\n",
    "We will now complete the parser and train it on our data. It is important to understand the difference between the following tasks:\n",
    "\n",
    "* Training: Training the model involves passing it sentences along with the correct sequence of actions, and updating weights.\n",
    "* Evaluation: We can evaluate the parser by passing it sentences along with the correct sequence of actions, and see how many actions it predicts correctly.  This is identical to training, except the weights are not updated after making a prediction.\n",
    "* Prediction: After setting the weights, we give it a raw sentence (no gold-standard actions), and let it follow its own predicted actions to create a dependency graph, which we can compare to the ground truth.\n",
    "\n",
    "You will implement the `forward()` function in `mynlplib.parsing.TransitionParser`.\n",
    "\n",
    "At this point, it is necessary to have all of the components from part 2 in place for constructing the parser.\n",
    "\n",
    "The parsing logic is roughly as follows:\n",
    "* Loop until parsing state is in its terminating state (deliverable 1.2)\n",
    "* Get the features from the parsing state (deliverable 2.2)\n",
    "* Send them through your action chooser network to get log probabilities over actions (deliverable 2.3)\n",
    "* If you have `gold_actions`, do them. Otherwise (when predicting), take the argmax of your log probabilities, validate the action (deliverable 1.3), and do that. An argmax function is provided for you in `utils.argmax`.\n",
    "\n",
    "Make sure to keep track of the things that the function wants to keep track of\n",
    "* Do all of your actions by calling the appropriate function on your `parser_state`\n",
    "* Append each output autograd.Variable from your action_chooser to the outputs list\n",
    "* Append each action you do to `actions_done`\n",
    "* Build the set of dependency edges as you go\n",
    "\n",
    "**Tests:**\n",
    "- `test_parser.py:test_parse_logic_d3_1`\n",
    "- `test_parser.py:test_predict_after_train_d3_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"The man ran away\".split()\n",
    "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
    "test_word_to_ix[consts.END_OF_INPUT_TOK] = len(test_word_to_ix)\n",
    "test_sentence_vocab = set(test_sentence)\n",
    "gold_actions = [\"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_R\", \"ARC_R\", \"SHIFT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{DepGraphEdge(head=('<ROOT>', -1), modifier=('ran', 2)), DepGraphEdge(head=('ran', 2), modifier=('man', 1)), DepGraphEdge(head=('man', 1), modifier=('The', 0)), DepGraphEdge(head=('ran', 2), modifier=('away', 3))}\n",
      "[0, 1, 0, 1, 0, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup = neural_net.VanillaWordEmbedding(test_word_to_ix, STACK_EMBEDDING_DIM)\n",
    "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                     action_chooser, combiner_network)\n",
    "output, depgraph, actions_done = parser(test_sentence, gold_actions)\n",
    "print(depgraph)\n",
    "print(actions_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Train the Parser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your parser may take some time. There are 10,000 training sentences. We take a subset of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parser(parser, optimizer, dataset, n_epochs=1, n_train_insts=1000):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}\".format(epoch+1))\n",
    "\n",
    "        parser.train() # turn on dropout layers if they are there\n",
    "        parsing.train(dataset.training_data[:n_train_insts], parser, optimizer, verbose=True)\n",
    "\n",
    "        print(\"Dev Evaluation\")\n",
    "        parser.eval() # turn them off for evaluation\n",
    "        parsing.evaluate(dataset.dev_data, parser, verbose=True)\n",
    "        print(\"F-Score: {}\".format(evaluation.compute_metric(parser, dataset.dev_data, evaluation.fscore)))\n",
    "        print(\"Attachment Score: {}\".format(evaluation.compute_attachment(parser, dataset.dev_data)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup = neural_net.VanillaWordEmbedding(word_to_ix_en, STACK_EMBEDDING_DIM)\n",
    "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                     action_chooser, combiner_network)\n",
    "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.7078163771712159  Loss: 32.37324492931366\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.8438792390405294  Loss: 18.007621763944627\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9011579818031431  Loss: 12.509024324715137\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.934449958643507  Loss: 7.615389933176338\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9611248966087675  Loss: 5.287574468702078\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9739454094292804  Loss: 3.574098753067665\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9793217535153019  Loss: 2.704069236360956\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.989454094292804  Loss: 1.5278681585029699\n",
      "1.79 s ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.manual_seed(1)\n",
    "parsing.train(en_dataset.training_data[:100], parser, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.8259425493716338  Loss: 20.41987961708894\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.8233623627413859  Loss: 15.728462156808067\n",
      "F-Score: 0.464984839911928\n",
      "Attachment Score: 0.43910135049854854\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the parser for a while here.\n",
    "# Shouldn't take *too* long, even on a laptop\n",
    "torch.manual_seed(1)\n",
    "train_parser(parser, optimizer, en_dataset, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.2: Test Data Predictions (2 points)\n",
    "Run the code below to output your predictions on the test data and dev data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us.\n",
    "\n",
    "**Test**: `test_parser.py:test_dev_d3_2_english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.EN_D3_2_DEV_FILENAME, parser, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.EN_D3_2_TEST_FILENAME, parser, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.3: Dependency parsing in Norwegian (2 points)\n",
    "Run the code below to output your predictions on the **norwegian** test data and dev data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us.\n",
    "\n",
    "**Test**: `test_parser.py:test_dev_d3_3_norwegian`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup_nr = neural_net.VanillaWordEmbedding(word_to_ix_nr, STACK_EMBEDDING_DIM)\n",
    "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network_nr = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
    "                                     action_chooser_nr, combiner_network_nr)\n",
    "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8113890504815461  Loss: 13.949815350351855\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8226229099076616  Loss: 13.585948317384203\n",
      "F-Score: 0.4521501957801757\n",
      "Attachment Score: 0.4361118043424008\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8785792773576369  Loss: 9.363433301108715\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8264287496880459  Loss: 15.430829859188417\n",
      "F-Score: 0.4660331380773358\n",
      "Attachment Score: 0.4472173696031944\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=2, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(evaluation)\n",
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.NR_D3_3_DEV_FILENAME, parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.NR_D3_3_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Training Improvements (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.1: BiLSTM Word Embeddings (3 points)\n",
    "Implement the class `BiLSTMWordEmbedding` in `neural_net.py`.\n",
    "This class can replace your `VanillaWordEmbedding`.\n",
    "This class implements a sequence model over the sentence, where the t'th word's embedding is the hidden state at timestep t.\n",
    "This means that, rather than have our embeddings on the stack only include the semantics of a single word, our embeddings will contain information from all parts of the sentence (the LSTM will, in principle, learn what information is relevant).\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_bilstm_word_embeds_d4_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2 \n",
      "\n",
      "Embedding for Noam:\n",
      " tensor([[-6.3007e-02,  2.4330e-01, -7.0760e-02, -1.1852e-01,  1.8881e-01,\n",
      "         -1.6543e-01, -2.1600e-02,  1.4040e-02, -6.8058e-02, -1.8666e-01,\n",
      "          1.0207e-01,  2.2894e-02, -5.8540e-02, -6.3337e-02, -2.9607e-01,\n",
      "         -2.0053e-02, -1.8389e-01, -9.1271e-02, -5.1386e-02, -3.4879e-01,\n",
      "         -3.8826e-02,  8.8795e-02, -3.8836e-02,  1.2170e-02,  4.6013e-02,\n",
      "         -1.3923e-01,  1.9091e-02,  7.1751e-02,  9.5653e-02, -3.5629e-01,\n",
      "          1.9788e-01,  2.9786e-02,  6.1633e-02,  4.7286e-02, -2.9223e-01,\n",
      "         -7.4602e-02,  2.4812e-01, -1.3309e-01,  4.2635e-02,  4.2023e-02,\n",
      "          3.1180e-02,  5.5482e-03, -1.1297e-01,  1.4215e-02, -1.0769e-01,\n",
      "         -1.4725e-01, -7.3080e-02,  2.1588e-02,  1.7645e-01,  4.3659e-02,\n",
      "         -2.4070e-04,  1.1204e-02, -2.2866e-01,  1.1086e-01, -3.3928e-02,\n",
      "         -1.3846e-01, -8.5202e-03,  8.6117e-02,  9.5097e-02, -1.2923e-01,\n",
      "         -2.7905e-03, -6.9797e-02,  1.6902e-01, -1.0969e-01, -1.3452e-01,\n",
      "          1.5670e-01,  7.2735e-02, -2.0805e-01, -1.5710e-01, -1.4324e-01,\n",
      "         -7.5958e-02,  1.8515e-01,  4.4450e-02,  7.9908e-02, -1.2914e-01,\n",
      "         -1.5535e-01,  3.2856e-02, -1.4755e-01,  5.6925e-02, -9.1804e-02,\n",
      "         -1.3480e-01, -1.8542e-02,  1.4489e-01,  9.3584e-02,  6.2046e-02,\n",
      "         -9.7762e-02,  8.8054e-02,  6.1904e-02,  1.5637e-02,  5.7491e-02,\n",
      "          3.4077e-02, -1.6989e-01,  3.2679e-03, -1.2580e-01,  7.0331e-02,\n",
      "         -9.3819e-02, -1.1116e-01, -1.8796e-01,  3.5303e-02, -1.0488e-01]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "test_sentence = \"Noam Chomsky\".split()\n",
    "test_word_to_ix = { \"Noam\": 0, \"Chomsky\": 1 }\n",
    "\n",
    "lstm_word_embedder = neural_net.BiLSTMWordEmbedding(test_word_to_ix,\n",
    "                                                    WORD_EMBEDDING_DIM,\n",
    "                                                    STACK_EMBEDDING_DIM,\n",
    "                                                    num_layers=LSTM_NUM_LAYERS,\n",
    "                                                    dropout=DROPOUT)\n",
    "    \n",
    "lstm_embeds = lstm_word_embedder(test_sentence)\n",
    "print(type(lstm_embeds))\n",
    "print(len(lstm_embeds), \"\\n\")\n",
    "print(\"Embedding for Noam:\\n {}\".format(lstm_embeds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.2: Suffix Embeddings (3 points)\n",
    "We can also try to more explicitly include morphological information by embedding the suffix of a word in addition to the word itself. We approximate the \"suffix\" by just looking at the last two characters of a word.\n",
    "\n",
    "First, implement the function `build_suff_to_ix` in `utils.py`. It should take in a `word_to_ix` lookup and return a `suff_to_ix` lookup.\n",
    "\n",
    "Then, implement the class `SuffixAndWordEmbedding` in `neural_net.py`.\n",
    "This class embeds the words and suffixes in a sentence and then concatenates them to form one embedding. \n",
    "\n",
    "**Test**: `tests/test_parser.py:test_suff_word_embeds_d4_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "suff_to_ix_en = utils.build_suff_to_ix(word_to_ix_en)\n",
    "suff_to_ix_nr = utils.build_suff_to_ix(word_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 849)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suff_to_ix_en), len(suff_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "test_sentence = \"prefix fixsuf fixinfix\".split()\n",
    "test_word_to_ix = { \"prefix\": 0, \"fixsuf\": 1, \"fixinfix\": 2 }\n",
    "test_suff_to_ix = utils.build_suff_to_ix(test_word_to_ix)\n",
    "\n",
    "suff_word_embedder = neural_net.SuffixAndWordEmbedding(test_word_to_ix, test_suff_to_ix, TEST_EMBEDDING_DIM)\n",
    "test_embs = suff_word_embedder(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6614,  0.2669, -1.5228,  0.3817]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.3: Pretrained Embeddings (2 points)\n",
    "\n",
    "Fill in the function `initialize_with_pretrained` in `utils.py`.\n",
    "\n",
    "It will take a word embedding lookup component and initialize its lookup table with pretrained embeddings, which are provided. Note that this is only applicable for the Vanilla, BiLSTM, and SuffixAndWord embedding components.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_pretrained_embeddings_d4_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12429751455783844, -0.11472601443529129, -0.5684014558792114, -0.396965891122818, 0.22938089072704315]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pretrained_embeds = pickle.load(open(consts.PRETRAINED_EMBEDS_FILE, 'rb'))\n",
    "print(pretrained_embeds['four'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "embedder = neural_net.VanillaWordEmbedding(word_to_ix_en,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5776,  0.2705,  0.4988,  0.1272,  0.6237], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.forward(['four'])[0][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1243, -0.1147, -0.5684, -0.3970,  0.2294], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(utils);\n",
    "utils.initialize_with_pretrained(pretrained_embeds,embedder)\n",
    "print(embedder.forward(['four'])[0][0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.4: Better Arc Component Combination (3 points)\n",
    "Before, in order to combine two embeddings during an arc- operation, we just passed them through a feed-forward network and got a dense output.  Now, we will instead use a sequence model of the stack.  The combined embedding from an arc- operation is the next time step of an LSTM.  Implement `neural_net.LSTMCombiner`.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_lstm_combiner_d4_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "combiner = neural_net.LSTMCombiner(TEST_EMBEDDING_DIM,\n",
    "                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                          dropout=DROPOUT)\n",
    "head_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))\n",
    "mod_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0532, -0.1534,  0.1484, -0.0595]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = combiner(head_feat, mod_feat)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.5: Better action choosing (3 points)\n",
    "Instead of choosing the action from the combiner output independently at each time step, let's use an LSTM to predict the action. This way, past actions can influence the current decision directly. \n",
    "\n",
    "Implement `neural_net.LSTMActionChooser`. Use a linear layer to predict the action from the LSTM hidden state.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_lstm_action_chooser_d4_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "action_chooser = neural_net.LSTMActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES,\n",
    "                                                     LSTM_NUM_LAYERS,\n",
    "                                                     dropout=DROPOUT)\n",
    "feats = [ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0328, -1.1798, -1.0887]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = action_chooser(feats)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with the new components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(neural_net)\n",
    "reload(parsing)\n",
    "reload(feat_extractors)\n",
    "torch.manual_seed(1)\n",
    "stack_dim = STACK_EMBEDDING_DIM\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "# BiLSTM word embeddings will probably work best, but feel free to experiment with the others you developed\n",
    "word_embedding_lookup = neural_net.BiLSTMWordEmbedding(word_to_ix_en,\n",
    "                                                       WORD_EMBEDDING_DIM,\n",
    "                                                       STACK_EMBEDDING_DIM,\n",
    "                                                       num_layers=LSTM_NUM_LAYERS,\n",
    "                                                       dropout=DROPOUT)\n",
    "utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
    "action_chooser = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
    "                                              LSTM_NUM_LAYERS,\n",
    "                                              dropout=DROPOUT)\n",
    "combiner = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
    "                                   num_layers=LSTM_NUM_LAYERS,\n",
    "                                   dropout=DROPOUT)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                  action_chooser, combiner)\n",
    "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.796229802513465  Loss: 20.93961408064142\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.8564937523665278  Loss: 10.963262027847076\n",
      "F-Score: 0.584904339949434\n",
      "Attachment Score: 0.5598889309604947\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.8833034111310593  Loss: 12.569501896796748\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.88034835289663  Loss: 9.406927095454728\n",
      "F-Score: 0.6343035073922545\n",
      "Attachment Score: 0.6134040136312003\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The LSTMs will make this take longer, probably just a few minutes\n",
    "train_parser(parser, optimizer, en_dataset, n_epochs=2, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.6: Test Predictions: English (2 point)\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_dev_preds_d4_6_english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.EN_D4_6_DEV_FILENAME, parser, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.EN_D4_6_TEST_FILENAME, parser, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.7: Test Predictions: Norwegian (2 point)\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_dev_preds_d4_7_norwegian`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
    "# BiLSTM word embeddings will probably work best, but feel free to experiment with the others you developed\n",
    "word_embedding_lookup_nr = neural_net.BiLSTMWordEmbedding(word_to_ix_nr,\n",
    "                                                          WORD_EMBEDDING_DIM,\n",
    "                                                          STACK_EMBEDDING_DIM,\n",
    "                                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                                          dropout=DROPOUT)\n",
    "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_nr = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
    "                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                          dropout=DROPOUT)\n",
    "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
    "                                  action_chooser_nr, combiner_nr)\n",
    "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8142653997802339  Loss: 13.2001195525378\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8396556026952833  Loss: 11.748504309730972\n",
      "F-Score: 0.5122631285979322\n",
      "Attachment Score: 0.48290491639630645\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8847197983323638  Loss: 8.464239425085484\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8597454454704268  Loss: 10.622585836366682\n",
      "F-Score: 0.5689138768007578\n",
      "Attachment Score: 0.540678812078862\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.9161657294292548  Loss: 6.277243318439927\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8652358372847517  Loss: 10.927354865651225\n",
      "F-Score: 0.5818803188918262\n",
      "Attachment Score: 0.5481657100074869\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=3, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.NR_D4_7_DEV_FILENAME, parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.NR_D4_7_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bakeoff (24 points)\n",
    "\n",
    "We will have another bakeoff for this problem set.\n",
    "\n",
    "Try to implement new features and tune your network's architecture and hyperparameters to get the best network.\n",
    "Section 3 of [this paper](https://arxiv.org/pdf/1206.5533.pdf) may help out with hyper parameter tuning if you are new to neural networks.\n",
    "To get very competitive, it may be necessary to train for a large amount of time (leaving it running overnight should be fine).  Here are some suggestions.\n",
    "* Tune your learning rate.\n",
    "* Tune your other hyperparameters.\n",
    "* Try different optimizers.  torch.optim has a ton of different training algorithms.  SGD was used in this pset because it is fast, but it is the most vanilla of them.  Trying new ones, like Adam, will almost certainly boost performance\n",
    "* Try adding regularization to your network if you see evidence that it is overfitting. This can be done with:\n",
    "  * L2 regularization using the [weight decay argument](http://pytorch.org/docs/master/optim.html#torch.optim.SGD)\n",
    "  * adding dropout (already an input argument to some of the neural net components)\n",
    "  * implement early stopping (stop training if dev performance on some metric doesn't improve for k epochs)\n",
    "* Try customizing any of the 3 components (word embeddings, action choosing, combining) in clever ways.  You can create new classes that expose the same public interface and use them here (just leave your required ones untouched). Building word embeddings from characters using an RNN or convolutional layer may help.\n",
    "* Try new features.  Write new classes that expose the same public interface as SimpleFeatureExtractor.  Try looking further into stack history, or more input buffer lookahead, or features based on the action sequence. The possibilities are endless.\n",
    "* Within our currect interface, you can use any neural nets and static pretrained word embeddings. However, if you use pretrained contextualized embeddings from large data (e.g., BERT), you will not receive extra credits if your model ranks high. You can still try creative modifications and get extra credits for \"creative solutions\".\n",
    "\n",
    "**Tests**: \n",
    "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_1_english`\n",
    "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_2_norwegian`\n",
    "\n",
    "**Rubric**:\n",
    "English dev:\n",
    "- $\\geq$ 0.76: 2 points\n",
    "- $\\geq$ 0.77: 4 points\n",
    "- $\\geq$ 0.78: 6 points\n",
    "\n",
    "English test:\n",
    "- $\\geq$ 0.72: 2 points\n",
    "- $\\geq$ 0.73: 4 points\n",
    "- $\\geq$ 0.74: 6 points\n",
    "\n",
    "Norwegian dev:\n",
    "- $\\geq$ 0.71: 2 points\n",
    "- $\\geq$ 0.72: 4 points\n",
    "- $\\geq$ 0.73: 6 points\n",
    "\n",
    "Norwegian test:\n",
    "- $\\geq$ 0.70: 2 points\n",
    "- $\\geq$ 0.71: 4 points\n",
    "- $\\geq$ 0.72: 6 points\n",
    "\n",
    "**Extra credit**:\n",
    "\n",
    "For both languages:\n",
    "- Top five test performance in the class (4 points bonus)\n",
    "- We'll also give 4 bonus points to particularly unique / creative / well-motivated solutions (with motivation to be included in the write-up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cuda\n",
    "You can use CUDA to train your network, and you should expect decent speedup if you have a GPU and the CUDA toolkit installed.\n",
    "If you want to use CUDA in this assignment, change the HAVE_CUDA variable to True in constants.py, and call `.to_cuda()` on your parser. You may also need to reconfigure your Embedding layers if you didn't consider cuda before.\n",
    "\n",
    "We are not officially supporting CUDA though. If you have problems installing or running CUDA, perhaps just use the CPU. You can post on Piazza but we cannot guarantee to help you debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your hyperparameters here\n",
    "# e.g learning rate, regularization, lr annealing, dimensionality of embeddings, number of epochs, early stopping etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your parser here\n",
    "# name your TransitionParser bakeoff_parser to output your predictions below\n",
    "# bakeoff_parser_en = TransitionParser(...)\n",
    "\n",
    "# Also, choose an optimizer.\n",
    "# bakeoff_optimizer_en = optim...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for bakeoff\n",
    "train_parser(bakeoff_parser_en, bakeoff_optimizer_en, en_dataset, n_epochs=5, n_train_insts=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dev output\n",
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(\"bakeoff-dev-en.preds\", bakeoff_parser_en, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev performance\n",
    "evaluation.compute_output_attachment(\"bakeoff-dev-en.preds\", consts.EN_DEV_GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write test output\n",
    "evaluation.output_preds(\"bakeoff-test-en.preds\", bakeoff_parser_en, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make your norwegian parser if necessary\n",
    "# name your TransitionParser bakeoff_parser to output your predictions below\n",
    "# bakeoff_parser_nr = TransitionParser(...)\n",
    "\n",
    "# Also, choose an optimizer.\n",
    "# bakeoff_optimizer_nr = optim...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for bakeoff\n",
    "train_parser(bakeoff_parser_nr, bakeoff_optimizer_nr, nr_dataset, n_epochs=5, n_train_insts=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dev output\n",
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(\"bakeoff-dev-nr.preds\", bakeoff_parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev performance\n",
    "evaluation.compute_output_attachment(\"bakeoff-dev-nr.preds\", consts.NR_DEV_GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write test output\n",
    "evaluation.output_preds(\"bakeoff-test-nr.preds\", bakeoff_parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Writeup (22 points)\n",
    "\n",
    "You can start your write-up in any format you prefer (e.g., LaTeX, Markdown), but please remember to export to `pset3-writeup.pdf` upon submission. Also, you will be asked to post your Deliverable 6.2 and 6.3 on Piazza after the due date (plus late days).\n",
    "\n",
    "**Deliverable 6.1** (6 points):\n",
    "\n",
    "Consider a sentence of 10 words:\n",
    "- In Project 1, we learned to build a text classifier. If the classifier is binary, there are $2$ possible outputs for this sentence.\n",
    "- In Project 2, we learned to build a sequence tagger. If each word has four possible labels, there are $4^{10} = 1048576$ possible outputs for this sentence.\n",
    "- In this project, we learned to build a parser for **projective** dependency trees. **How many possible projective dependency trees are there for our sentence of 10 words? Why? Is this output space larger than the one above?**\n",
    "\n",
    "**Deliverable 6.2** (6 points):\n",
    "\n",
    "Briefly describe your bakeoff design.\n",
    "\n",
    "**Deliverable 6.3** (10 points):\n",
    "\n",
    "You will select a research paper at ACL, EMNLP or NAACL that **uses dependency trees for some downstream task**. Summarize the paper, answering the following questions:\n",
    "\n",
    "1. What is the task that is being solved?\n",
    "2. Briefly (one sentence) explain the metric for success on this task.\n",
    "3. Why are dependency features expected to help with this task?\n",
    "4. How are dependency features incorporated into the solution?\n",
    "5. Does the paper evaluate whether dependency features improve performance on the downstream task? If so, what is their impact? If not, why not?\n",
    "\n",
    "You must choose a paper in the main conference (not workshops). The paper must be at least four pages long. All papers from these conferences are available for free online: https://www.aclweb.org/anthology/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
